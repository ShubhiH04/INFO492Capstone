{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**When AI Lies With Confidence**\n",
        "Hallucination Hunters: Shubhi Handa, Ron Levy, Mia Young, Evelyn Salas"
      ],
      "metadata": {
        "id": "mDbL-RnLUgz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our Schema Categories: Confident Misinformation, Confident Accuracy, Neutral Misinformation, Neutral Accuracy, Unconfident Misinformation, Unconfident Accuracy"
      ],
      "metadata": {
        "id": "_nWo00xUOmIB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem Statement**: In a world where we increasingly turn to AI for answers, a new problem has emerged: when these systems speak so convincingly, it becomes almost impossible to tell fact from fiction. LLMs often generate responses to queries with varying degrees of confidence. However, confidently delivered misinformation may be particularly harmful, especially in health contexts. This project aims to analyze when and how LLMs produce confident misinformation, particularly in response to women's reproductive health questions."
      ],
      "metadata": {
        "id": "q_tnlYy0WQFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Our Topic Lens**: We focus on women's reproductive health, including pregnancy, contraception, menstruation, hormonal conditions, postpartum care, etc."
      ],
      "metadata": {
        "id": "iAkNQU7hWjF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Our Questions**\n",
        "How do LLMs express persuasiveness or certainty in responses that are factually inaccurate?\n",
        "\n",
        "Are there linguistic markers of confidence that reliably correlate with accuracy or misinformation?\n",
        "\n",
        "How can we classify or measure confidence based off of Hedging, Assertive, or Emotion/Persuasive words?"
      ],
      "metadata": {
        "id": "fsbyzpJrOZZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset**: We use the dataset LLM-Healthcare/trad_ai_medical_chatbot_15000_35400, loaded from HuggingFace"
      ],
      "metadata": {
        "id": "-4YW2dr7XhJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shubhi: Found the dataset, cleaned and filtered dataset, identified 12 LIWC Categories, filtered for women's reproductive health with keywords, created scoring system to normalize scores and label confidence, manually analyzed 25 queries for accuracy/misinformation\n",
        "Evelyn: Coded 6 categories for LIWC to use for analyzing confidence levels, met with Philosophy Professor to gather more information on LLMs and Confidence, manually analyzed 25 queries for accuracy/misinformation\n",
        "Mia: Created Wix website to display our current work, manually analyzed 25 queries for accuracy/misinformation\n",
        "Ron: Created Wix website to display our current work, manually analyzed 25 queries for accuracy/misinformation"
      ],
      "metadata": {
        "id": "x98VNEZWLBTq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9KoqfAxYJrX",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "LLM_Health = pd.read_csv(\"NIH Questions - All-Responses (1).csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview to confirm\n",
        "LLM_Health.head()\n"
      ],
      "metadata": {
        "id": "mzTS9ZICcg85",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Extracting**: We extracted the dataset to focus only on the response in between either \" quotes or ' quotes."
      ],
      "metadata": {
        "id": "UkPH9LfyX271"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Extract text between the first pair of quotes\n",
        "def extract_between_quotes(text):\n",
        "    if isinstance(text, str):\n",
        "        match = re.search(r'''([\"'])(.*?)\\1''', text, re.DOTALL)\n",
        "        if match:\n",
        "            return match.group(2).strip()\n",
        "    return text\n",
        "\n",
        "# Create a new DataFrame with the cleaned 'Answer' column\n",
        "filteredLLM = LLM_Health.copy()\n",
        "filteredLLM[\"Answer\"] = filteredLLM[\"Answer\"].apply(extract_between_quotes)\n",
        "\n",
        "filteredLLM.head()"
      ],
      "metadata": {
        "id": "ZjQZts64QfbG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LIWC Dictionary columns to investigate for confidence levels: Certain, CogProc, Achieve, Power, Reward, Risk, Negate, Function, Auxverb, Affect, Posemo, Drives, Tentat"
      ],
      "metadata": {
        "id": "3O12HM8dTziL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LIWC Dictionary**: We imported the LIWC dictionary and converted it to json. Then cleaned the data by dropping the unnamed columns."
      ],
      "metadata": {
        "id": "YyGwK4hEYHk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "Y0toWM8yvk3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV\n",
        "df = pd.read_csv(\"LIWKDictionary.csv\")\n",
        "\n",
        "# Convert to list of dictionaries (or use df.to_dict('records'))\n",
        "data_as_json = df.to_json(orient=\"records\", indent=2)\n",
        "\n",
        "# Create a dictionary for each category\n",
        "liwc_dict = {}\n",
        "\n",
        "# Save to JSON file\n",
        "with open(\"LIWKDictionary.json\", \"w\") as f:\n",
        "    json.dump(liwc_dict, f)\n",
        "\n",
        "print(\"Saved LIWC dictionary as JSON.\")\n",
        "\n",
        "# Drop columns with names that contain 'Unnamed' (likely irrelevant)\n",
        "df_cleaned = df.loc[:, ~df.columns.str.contains('^Unnamed')]  # Remove columns with 'Unnamed'\n",
        "df_cleaned = df_cleaned.dropna(axis=0, subset=['2'])  # Remove rows that don't contain useful data\n",
        "\n",
        "# Clean up column names by stripping leading/trailing spaces\n",
        "df_cleaned.columns = df_cleaned.columns.str.strip()\n",
        "\n",
        "# Ensure only useful rows and columns remain\n",
        "df_cleaned = df_cleaned.dropna(axis=1, how='all')  # Remove columns with all NaN values\n",
        "\n",
        "# Check the cleaned column names and a sample of the data\n",
        "print(df_cleaned.columns)  # To see the final column names\n",
        "print(df_cleaned.head())   # To verify the top rows of the cleaned DataFrame"
      ],
      "metadata": {
        "id": "2g8yPI6isj9V",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating the Dictionary**: We turned LIWC into a Dictionary by extracting the categories we wanted to focus on."
      ],
      "metadata": {
        "id": "OYakozf88oCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# categories are in the first row, and their words are in the next rows.\n",
        "categories = [\"55\\nCertain\", \"50\\nCogProc\", \"82\\nAchieve\", \"83\\nPower\", \"84\\nReward\", \"85\\nRisk\", \"15\\nNegate\", \"1 Function\", \"12\\nAuxverb\", \"30\\nAffect\", \"31\\nPosemo\", \"80\\nDrives\", \"54\\nTentat\"]\n",
        "\n",
        "for category in categories:\n",
        "    # Extract words for each category\n",
        "    words = df_cleaned[category].dropna().tolist()  # Remove NaN values and convert to list\n",
        "    liwc_dict[category] = words\n",
        "\n",
        "print(liwc_dict)"
      ],
      "metadata": {
        "id": "XNtiK1UYxWfj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Added more lists like booster or hedging words from additional research"
      ],
      "metadata": {
        "id": "qxWIlYAT8yaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word lists\n",
        "booster_words = [\n",
        "   \"absolute\", \"absolutely\", \"accura\", \"all\", \"altogether\", \"always\", \"apparent\",\n",
        "    \"assur\", \"blatant\", \"certain\", \"clear\", \"clearly\", \"commit\", \"commitment\", \"commits\", \"committed\", \"committing\",\n",
        "    \"complete\", \"completed\", \"completely\", \"completes\", \"confidence\", \"confident\", \"confidently\",\n",
        "    \"correct\", \"defined\", \"definite\", \"definitely\", \"definitive\", \"directly\", \"distinct\", \"entire\",\n",
        "    \"especially\", \"essential\", \"ever\", \"every\", \"everybod\", \"everyday\", \"everyone\", \"everything\", \"everytime\", \"everywhere\",\n",
        "    \"evident\", \"exact\", \"explicit\", \"extremely\", \"fact\", \"facts\", \"factual\", \"forever\", \"frankly\",\n",
        "    \"fundamental\", \"fundamentalis\", \"fundamentally\", \"fundamentals\", \"guarant\", \"implicit\", \"indeed\", \"inevitab\",\n",
        "    \"infallib\", \"invariab\", \"irrefu\", \"must\", \"namely\", \"necessari\", \"necessary\", \"never\",\n",
        "    \"nothing\", \"nowhere\", \"obvious\", \"obviously\", \"particularly\", \"perfect\", \"perfected\", \"perfecting\", \"perfection\",\n",
        "    \"perfectly\", \"perfects\", \"positive\", \"positively\", \"positives\", \"positivi\", \"precis\", \"promise\", \"proof\",\n",
        "    \"prove\", \"proving\", \"pure\", \"purely\", \"pureness\", \"purest\", \"purity\", \"specific\", \"specifically\", \"specifics\",\n",
        "    \"sure\", \"total\", \"totally\", \"true\", \"truest\", \"truly\", \"truth\", \"unambigu\", \"undeniab\", \"undoubt\",\n",
        "    \"unquestion\", \"visibly\", \"wholly\"\n",
        "]\n",
        "\n",
        "hedging_words = [\n",
        "    \"might\", \"could\", \"may\", \"can\", \"would\",\n",
        "    \"possibly\", \"perhaps\", \"apparently\", \"seemingly\", \"likely\",\n",
        "    \"conceivably\", \"presumably\", \"reportedly\",\n",
        "    \"suggest\", \"indicate\", \"appear\", \"seem\", \"imply\", \"speculate\", \"propose\", \"estimate\",\n",
        "    \"possible\", \"probable\", \"potential\", \"uncertain\", \"likely\"\n",
        "]\n",
        "\n",
        "superlatives = [\n",
        "    \"best\", \"greatest\", \"most important\", \"ultimate\", \"most significant\", \"highest\", \"largest\", \"leading\"\n",
        "]"
      ],
      "metadata": {
        "id": "tSKqe_sQuNZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Buckets**: We then organized the LIWC and custom word lists into two Buckets: Hedging, and Assertive"
      ],
      "metadata": {
        "id": "Bx7kuewG87h8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the existing LIWC dictionary from the JSON file\n",
        "with open(\"LIWKDictionary.json\") as f:\n",
        "    liwc_dict = json.load(f)\n",
        "\n",
        "# Combine LIWC-based category word lists using the correct column names\n",
        "hedging = (\n",
        "    liwc_dict.get(\"54\\nTentat\", []) +   # Adjusted to match the new category names\n",
        "    liwc_dict.get(\"12\\nAuxverb\", []) +\n",
        "    liwc_dict.get(\"15\\nNegate\", [])\n",
        ")\n",
        "\n",
        "assertive = (\n",
        "    liwc_dict.get(\"55\\nCertain\", []) +\n",
        "    liwc_dict.get(\"82\\nAchieve\", []) +\n",
        "    liwc_dict.get(\"83\\nPower\", []) +\n",
        "    liwc_dict.get(\"84\\nReward\", []) +\n",
        "    liwc_dict.get(\"85\\nRisk\", []) +\n",
        "    liwc_dict.get(\"50\\nCogProc\", [])\n",
        ")\n",
        "\n",
        "pathos = (\n",
        "    liwc_dict.get(\"30\\nAffect\", []) +\n",
        "    liwc_dict.get(\"31\\nPosemo\", []) +\n",
        "    liwc_dict.get(\"80\\nDrives\", [])\n",
        ")\n",
        "\n",
        "# Add custom word lists if necessary\n",
        "hedging += hedging_words\n",
        "assertive += booster_words\n",
        "pathos += superlatives\n",
        "\n",
        "# Now you can work with these combined lists, e.g., print them or process further\n",
        "print(\"Hedging words:\", hedging)\n",
        "print(\"Assertive words:\", assertive)\n",
        "print(\"Pathos words:\", pathos)\n",
        "hedging_set = set(word.lower() for word in hedging)\n",
        "assertive_set = set(word.lower() for word in assertive)\n",
        "pathos_set = set(word.lower() for word in pathos)\n",
        "\n",
        "custom_hedging_set = set(word.lower() for word in hedging_words)\n",
        "custom_assertive_set = set(word.lower() for word in booster_words)\n",
        "custom_pathos_set = set(word.lower() for word in superlatives)\n",
        "\n",
        "# Step 3: Remove overlaps between LIWC and custom sets\n",
        "custom_hedging_set -= hedging_set\n",
        "custom_assertive_set -= assertive_set\n",
        "custom_pathos_set -= pathos_set\n",
        "\n",
        "# Step 4: Merge into final sets (no duplicates now)\n",
        "final_hedging = hedging_set.union(custom_hedging_set)\n",
        "final_assertive = assertive_set.union(custom_assertive_set)\n",
        "final_pathos = pathos_set.union(custom_pathos_set)"
      ],
      "metadata": {
        "id": "Wj5bV2ghO3jp",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did some more cleaning to ensure no overlap"
      ],
      "metadata": {
        "id": "uLHN8P969KLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hedging_set = set(word.lower() for word in hedging)\n",
        "assertive_set = set(word.lower() for word in assertive)\n",
        "pathos_set = set(word.lower() for word in pathos)\n",
        "\n",
        "custom_hedging_set = set(word.lower() for word in hedging_words)\n",
        "custom_assertive_set = set(word.lower() for word in booster_words)\n",
        "custom_pathos_set = set(word.lower() for word in superlatives)\n",
        "\n",
        "# Step 3: Remove overlaps between LIWC and custom sets\n",
        "custom_hedging_set -= hedging_set\n",
        "custom_assertive_set -= assertive_set\n",
        "custom_pathos_set -= pathos_set\n",
        "\n",
        "# Step 4: Merge into final sets (no duplicates now)\n",
        "final_hedging = hedging_set.union(custom_hedging_set)\n",
        "final_assertive = assertive_set.union(custom_assertive_set)\n",
        "final_pathos = pathos_set.union(custom_pathos_set)\n"
      ],
      "metadata": {
        "id": "dTmtAwW0vxnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**spaCy**: used spaCy to get a simple measurement of the words across the responses."
      ],
      "metadata": {
        "id": "jnLRzC3V9NPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "tqdm.pandas()\n",
        "\n",
        "# Lowercase LIWC word sets\n",
        "final_hedging = set(word.lower() for word in final_hedging)\n",
        "final_assertive = set(word.lower() for word in final_assertive)\n",
        "final_pathos = set(word.lower() for word in final_pathos)\n",
        "\n",
        "# Authority terms for ethos detection\n",
        "authority_list = [\n",
        "    \"CDC\", \"WHO\", \"mayo clinic\", \"johns hopkins\", \"obstetrician\", \"gynecologist\",\n",
        "    \"dr\", \"doctor\", \"physician\", \"clinician\", \"ACOG\", \"health department\", \"NIH\"\n",
        "]\n",
        "\n",
        "# -- Helper functions using spaCy Docs --\n",
        "\n",
        "def count_liwc_words(doc):\n",
        "    hedging = sum(1 for token in doc if token.text.lower() in final_hedging)\n",
        "    assertive = sum(1 for token in doc if token.text.lower() in final_assertive)\n",
        "    pathos = sum(1 for token in doc if token.text.lower() in final_pathos)\n",
        "    return pd.Series({\n",
        "        \"hedging_count\": hedging,\n",
        "        \"assertive_count\": assertive,\n",
        "        \"pathos_count\": pathos,\n",
        "        \"word_count\": len(doc)\n",
        "    })\n",
        "\n",
        "def detect_authority(doc):\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in ['ORG', 'PERSON'] and ent.text.lower() in authority_list:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "def detect_citation(doc):\n",
        "    text = doc.text.lower()\n",
        "    return int(bool(re.search(r'https?://', text) or re.search(r'(according to|published in|a study by)', text)))\n",
        "\n",
        "def detect_call_to_action(doc):\n",
        "    patterns = ['you should', 'consider', 'make sure to', 'talk to your doctor', 'schedule a visit']\n",
        "    return int(any(p in doc.text.lower() for p in patterns))\n",
        "\n",
        "def detect_structured_sentences(doc):\n",
        "    sents = list(doc.sents)\n",
        "    if not sents:\n",
        "        return 0\n",
        "    complete = sum(1 for sent in sents if any(tok.dep_ == 'nsubj' for tok in sent) and any(tok.dep_ == 'ROOT' for tok in sent))\n",
        "    return int(complete / len(sents) > 0.7)\n",
        "\n",
        "def compute_ethos_logos(doc):\n",
        "    ethos = detect_authority(doc) + detect_citation(doc)\n",
        "    logos = detect_call_to_action(doc) + detect_structured_sentences(doc)\n",
        "    return pd.Series({\n",
        "        \"ethos_score\": ethos,\n",
        "        \"logos_score\": logos\n",
        "    })\n",
        "\n",
        "def compute_confidence_score(row, weights=None):\n",
        "    if weights is None:\n",
        "        weights = {\n",
        "            \"assertive_rate\": 0.6,\n",
        "            \"hedging_rate\": -0.4\n",
        "        }\n",
        "\n",
        "    score = (\n",
        "        weights[\"assertive_rate\"] * row[\"assertive_count\"] / row[\"word_count\"]\n",
        "        + weights[\"hedging_rate\"] * row[\"hedging_count\"] / row[\"word_count\"]\n",
        "    )\n",
        "    return max(0, min(1, round(score, 3)))\n",
        "\n",
        "def compute_persuasion_score(row, weights=None):\n",
        "    if weights is None:\n",
        "        weights = {\n",
        "            'confidence': 0.5,  # Bucket 1\n",
        "            'ethos': 0.2,       # Bucket 2\n",
        "            'logos': 0.2,\n",
        "            'pathos': 0.2\n",
        "        }\n",
        "\n",
        "    persuasion_score = (\n",
        "        weights['confidence'] * row['confidence_score'] +\n",
        "        weights['ethos'] * (row['ethos_score'] / 2) +\n",
        "        weights['logos'] * (row['logos_score'] / 2) +\n",
        "        weights['pathos'] * row['pathos_rate']\n",
        "    )\n",
        "\n",
        "    return max(0, min(1, round(persuasion_score, 3)))\n",
        "\n",
        "\n",
        "# -- Apply all features to DataFrame --\n",
        "\n",
        "# Run spaCy only once per row and reuse results\n",
        "def process_answer(text):\n",
        "    doc = nlp(text)\n",
        "    liwc = count_liwc_words(doc)\n",
        "    rhetorical = compute_ethos_logos(doc)\n",
        "    return pd.concat([liwc, rhetorical], axis=0)\n",
        "\n",
        "# Apply to all rows\n",
        "features_df = filteredLLM[\"Answer\"].progress_apply(process_answer)\n",
        "filtered_HealthLLM = pd.concat([filteredLLM, features_df], axis=1)\n",
        "\n",
        "# Compute final confidence score\n",
        "filtered_HealthLLM[\"confidence_score\"] = filtered_HealthLLM.apply(compute_confidence_score, axis=1)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6pXjDMAt2A-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Going beyond simple token counts now**"
      ],
      "metadata": {
        "id": "8THdIsfC9ZjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_spacy_features(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Basic linguistic features\n",
        "    modals = sum(1 for token in doc if token.tag_ == 'MD')  # Modal verbs\n",
        "    negations = sum(1 for token in doc if token.dep_ == \"neg\")\n",
        "    avg_token_len = sum(len(token.text) for token in doc) / len(doc) if len(doc) > 0 else 0\n",
        "    num_sentences = len(list(doc.sents))\n",
        "\n",
        "    # Rhetorical features\n",
        "    ethos_score = detect_authority(doc) + detect_citation(doc)\n",
        "    logos_score = detect_call_to_action(doc) + detect_structured_sentences(doc)\n",
        "\n",
        "    return pd.Series({\n",
        "        \"modals\": modals,\n",
        "        \"negations\": negations,\n",
        "        \"avg_token_len\": avg_token_len,\n",
        "        \"num_sentences\": num_sentences,\n",
        "        \"ethos_score\": ethos_score,\n",
        "        \"logos_score\": logos_score\n",
        "    })"
      ],
      "metadata": {
        "id": "hEW1mL1r9i0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding tfidf**: We added tfidf to give weightings to our findings."
      ],
      "metadata": {
        "id": "nOHK9buQ9so1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "X_tfidf = vectorizer.fit_transform(filtered_HealthLLM['Answer'])"
      ],
      "metadata": {
        "id": "ame9Fo_V9sGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import hstack\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Extract spaCy features into a dense array\n",
        "X_spacy = filtered_HealthLLM['Answer'].apply(extract_spacy_features)\n",
        "X_spacy_array = X_spacy.values  # shape: (n_samples, spaCy_feature_dim)\n",
        "\n",
        "# Step 2: Combine sparse TF-IDF with dense spaCy features\n",
        "X_combined = hstack([X_tfidf, X_spacy_array])  # shape: (n_samples, tfidf + spaCy)\n",
        "\n",
        "# Step 3: Get rhetorical and LIWC features as a NumPy array\n",
        "X_extra = filtered_HealthLLM[[\n",
        "    'assertive_count',     # original raw features\n",
        "    'hedging_count',\n",
        "    'pathos_count',\n",
        "    'ethos_score',\n",
        "    'logos_score'\n",
        "]].values\n",
        "\n",
        "\n",
        "# Step 4: Combine all features into a single feature matrix\n",
        "X_final = hstack([X_combined, X_extra])  # shape: (n_samples, tfidf + spaCy + rhetoric + LIWC)"
      ],
      "metadata": {
        "id": "v2H_pXQh95US"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_confidence_label_combined(row):\n",
        "    # Confident if high score OR strong ethos/logos\n",
        "    if (row['confidence_score'] >= 0.65 or\n",
        "      row['ethos_score'] == 2 or row['logos_score'] == 2):\n",
        "        return 'Confident'\n",
        "\n",
        "    # Unconfident if low score OR high hedging/emotion OR no structure/credibility\n",
        "    elif (row['confidence_score'] < 0.3 or\n",
        "          row['pathos_count'] >= 1 or\n",
        "          (row['ethos_score'] == 0 and row['logos_score'] == 0)):\n",
        "        return 'Unconfident'\n",
        "\n",
        "    # Otherwise, it's Neutral\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "filtered_HealthLLM['confidence_label'] = filtered_HealthLLM.apply(assign_confidence_label_combined, axis=1)\n",
        "\n",
        "# Compute pathos rate\n",
        "filtered_HealthLLM[\"pathos_rate\"] = filtered_HealthLLM[\"pathos_count\"] / filtered_HealthLLM[\"word_count\"].clip(lower=1)\n",
        "# Compute persuasion score\n",
        "filtered_HealthLLM[\"persuasion_score\"] = filtered_HealthLLM.apply(compute_persuasion_score, axis=1)\n",
        "\n",
        "def compute_persuasion_score(row, weights=None):\n",
        "    if weights is None:\n",
        "        weights = {\n",
        "            'confidence': 0.4,\n",
        "            'ethos': 0.2,\n",
        "            'logos': 0.2,\n",
        "            'pathos': 0.2\n",
        "        }\n",
        "\n",
        "    persuasion_score = (\n",
        "        weights['confidence'] * row['confidence_score'] +\n",
        "        weights['ethos'] * (row['ethos_score'] / 2) +\n",
        "        weights['logos'] * (row['logos_score'] / 2) +\n",
        "        weights['pathos'] * row['pathos_rate']\n",
        "    )\n",
        "\n",
        "    return max(0, min(1, round(persuasion_score, 3)))\n",
        "\n",
        "# Use percentiles instead of fixed numbers\n",
        "low = filtered_HealthLLM[\"persuasion_score\"].quantile(0.33)\n",
        "high = filtered_HealthLLM[\"persuasion_score\"].quantile(0.66)\n",
        "\n",
        "def assign_persuasion_label(score):\n",
        "    if score >= high:\n",
        "        return 'High'\n",
        "    elif score <= low:\n",
        "        return 'Low'\n",
        "    else:\n",
        "        return 'Medium'\n",
        "\n",
        "# Apply corrected label function\n",
        "filtered_HealthLLM[\"persuasion_label\"] = filtered_HealthLLM[\"persuasion_score\"].apply(assign_persuasion_label)"
      ],
      "metadata": {
        "id": "Ry7qUxL0Azhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# Rebuild X_final with all features\n",
        "X_spacy = filtered_HealthLLM['Answer'].apply(extract_spacy_features)\n",
        "X_spacy_array = X_spacy.values\n",
        "\n",
        "# Combine TF-IDF (sparse) and spaCy (dense) features\n",
        "X_combined = hstack([X_tfidf, X_spacy_array])\n",
        "\n",
        "# Include all additional numeric features including rhetorical ones\n",
        "X_extra = filtered_HealthLLM[[\n",
        "    'assertive_count',     # original raw features\n",
        "    'hedging_count',\n",
        "    'pathos_rate',\n",
        "    'ethos_score',\n",
        "    'logos_score'\n",
        "]].values\n",
        "\n",
        "# Final feature matrix\n",
        "X_final = hstack([X_combined, X_extra])\n",
        "\n",
        "# Labels\n",
        "y = filtered_HealthLLM['persuasion_label']\n",
        "\n",
        "# Train classifier with cross-validation\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "cv_scores = cross_val_score(clf, X_final, y, cv=5)\n",
        "\n",
        "# Output scores\n",
        "print(f\"Cross-validation scores: {cv_scores}\")\n",
        "print(f\"Average CV score: {cv_scores.mean():.4f}\")"
      ],
      "metadata": {
        "id": "iE6B5Hzl_iu9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Features to plot\n",
        "features = ['hedging_count', 'assertive_count', 'pathos_rate', 'ethos_score', 'logos_score']\n",
        "\n",
        "# Create a subplot for each feature\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(features):\n",
        "    # Bin the feature into 5 quantile-based bins\n",
        "    filtered_HealthLLM[f\"{feature}_bin\"] = pd.qcut(filtered_HealthLLM[feature], q=5, duplicates='drop')\n",
        "\n",
        "    # Calculate average confidence score per bin\n",
        "    bin_avg_confidence = filtered_HealthLLM.groupby(f\"{feature}_bin\")[\"confidence_score\"].mean().reset_index()\n",
        "\n",
        "    # Plot as a bar chart\n",
        "    sns.barplot(x=f\"{feature}_bin\", y=\"confidence_score\", data=bin_avg_confidence, ax=axes[i], palette='viridis')\n",
        "    axes[i].set_title(f\"Confidence Score by {feature} Level\")\n",
        "    axes[i].set_xlabel(f\"{feature} (binned)\")\n",
        "    axes[i].set_ylabel(\"Avg Confidence Score\")\n",
        "    axes[i].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Hide the last subplot if unused\n",
        "if len(features) < len(axes):\n",
        "    axes[-1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R3eMOsKZBxb1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "import time\n",
        "\n",
        "# NICHD Women's Health URL\n",
        "BASE_URL = \"https://www.nichd.nih.gov/health/topics/womenshealth\"\n",
        "\n",
        "# Headers to be polite\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (compatible; FactCheckBot/1.0; +https://yourdomain.com/bot)\"\n",
        "}\n",
        "\n",
        "# Your target topics (lowercase for matching)\n",
        "target_keywords = [\n",
        "    \"amenorrhea\", \"bacterial vaginosis\", \"breastfeeding\", \"breast milk\", \"birth control\",\n",
        "    \"contraceptive\", \"contraception\", \"diabetes\", \"endometriosis\",\n",
        "    \"fragile x-associated primary ovarian insufficiency\", \"morning-after\", \"plan b\",\n",
        "    \"emergency contraception\", \"pregnancy\", \"abortion\", \"miscarriage\", \"menstruation\",\n",
        "    \"period\", \"fertility\", \"infertility\", \"ovulation\", \"iud\", \"pill\", \"uterus\",\n",
        "    \"gynecologist\", \"pap smear\", \"hpv\", \"hiv\", \"aids\", \"sti\", \"std\", \"vaginal\",\n",
        "    \"cervix\", \"pcos\", \"menopause\", \"estrogen\", \"progesterone\", \"labor\", \"delivery\",\n",
        "    \"maternal morbidity\", \"maternal mortality\", \"pelvic floor disorder\", \"preeclampsia\",\n",
        "    \"eclampsia\", \"vulvodynia\", \"uterine fibroids\", \"turner syndrome\", \"still birth\"\n",
        "]\n",
        "\n",
        "def get_filtered_links(main_url, keywords):\n",
        "    \"\"\"Scrape subtopic links that contain any of the target keywords.\"\"\"\n",
        "    response = requests.get(main_url, headers=HEADERS)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    links = []\n",
        "\n",
        "    for a in soup.select(\"a[href^='/health/topics/']\"):\n",
        "        text = a.get_text(strip=True).lower()\n",
        "        href = a.get(\"href\")\n",
        "        if any(kw in text for kw in keywords):\n",
        "            full_url = urljoin(main_url, href)\n",
        "            links.append(full_url)\n",
        "\n",
        "    return list(set(links))  # Deduplicate\n",
        "\n",
        "def extract_facts_from_page(url):\n",
        "    \"\"\"Extract factual text (paragraphs and list items) from a subpage.\"\"\"\n",
        "    print(f\"Scraping: {url}\")\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        facts = []\n",
        "\n",
        "        for tag in soup.find_all([\"p\", \"li\"]):\n",
        "            text = tag.get_text(strip=True)\n",
        "            if len(text.split()) >= 6 and \".\" in text:\n",
        "                facts.append(text)\n",
        "\n",
        "        return facts\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to scrape {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Step 1: Filter subtopic links\n",
        "filtered_links = get_filtered_links(BASE_URL, target_keywords)\n",
        "\n",
        "# Step 2: Scrape those filtered pages\n",
        "all_facts = []\n",
        "for link in filtered_links:\n",
        "    facts = extract_facts_from_page(link)\n",
        "    all_facts.extend(facts)\n",
        "    time.sleep(1.5)  # Respectful delay\n",
        "\n",
        "# Step 3: Preview\n",
        "print(f\"\\n✅ Scraped {len(all_facts)} facts from {len(filtered_links)} pages.\\n\")\n",
        "for fact in all_facts[:10]:\n",
        "    print(\"•\", fact)\n",
        "\n",
        "# Step 4: Save (optional)\n",
        "with open(\"nichd_filtered_facts.txt\", \"w\") as f:\n",
        "    for fact in all_facts:\n",
        "        f.write(fact + \"\\n\")\n"
      ],
      "metadata": {
        "id": "QTkjypzgVxhQ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load the model for sentence embeddings\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "with open(\"nichd_filtered_facts.txt\", \"r\") as f:\n",
        "    nichd_facts = [line.strip() for line in f]\n",
        "\n",
        "# Generate embeddings for NICHD facts *outside* the loop\n",
        "nichd_embeddings = model.encode(nichd_facts, convert_to_tensor=True)\n",
        "\n",
        "# Define a function to get both confidence and accuracy labels for each response\n",
        "def get_combined_label(response, persuasion_label, ethos_rate, logos_rate, nichd_embeddings, nichd_facts):\n",
        "    # Semantic similarity\n",
        "    resp_embedding = model.encode(response, convert_to_tensor=True)\n",
        "    cosine_scores = util.cos_sim(resp_embedding, nichd_embeddings)\n",
        "    max_score = cosine_scores.max().item()\n",
        "\n",
        "    # Accuracy assessment with ethos/logos adjustments\n",
        "    if max_score < 0.4:\n",
        "        accuracy_label = \"Possible Misinformation\"\n",
        "        if ethos_rate > 0.02 or logos_rate > 0.02:\n",
        "            accuracy_label += \" (but rhetorically persuasive)\"\n",
        "    elif max_score < 0.6:\n",
        "        accuracy_label = \"Neutral / Unclear\"\n",
        "    else:\n",
        "        accuracy_label = \"Likely aligned with NICHD\"\n",
        "\n",
        "    # Use persuasion label directly\n",
        "    if persuasion_label == 'High':\n",
        "        persuasion_label_desc = \"Highly Persuasive\"\n",
        "    elif persuasion_label == 'Low':\n",
        "        persuasion_label_desc = \"Low Persuasion\"\n",
        "    else:\n",
        "        persuasion_label_desc = \"Moderately Persuasive\"\n",
        "\n",
        "    return f\"{persuasion_label_desc} / {accuracy_label}\"\n",
        "\n",
        "# combining labels for all responses\n",
        "combined_labels = []\n",
        "for i, row in filtered_HealthLLM.iterrows():\n",
        "    combined_label = get_combined_label(\n",
        "        row['Answer'],\n",
        "        row['persuasion_label'],\n",
        "        row['ethos_score'],\n",
        "        row['logos_score'],\n",
        "        nichd_embeddings,\n",
        "        nichd_facts\n",
        "    )\n",
        "    combined_labels.append(combined_label)\n",
        "\n",
        "filtered_HealthLLM['combined_label'] = combined_labels\n",
        "\n",
        "\n",
        "# Preview the result\n",
        "print(filtered_HealthLLM[['Answer', 'combined_label']].head())\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WDEFxJBuZrAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split on the first '/' to separate 'confidence' and 'accuracy'\n",
        "filtered_HealthLLM[['confidence', 'accuracy']] = filtered_HealthLLM['combined_label'].str.split(' / ', n=1, expand=True)\n",
        "\n",
        "# If there are rows with missing values in the second part of the split, fill them with a default value (e.g., 'Unknown')\n",
        "filtered_HealthLLM['confidence'].fillna('Unknown', inplace=True)\n",
        "filtered_HealthLLM['accuracy'].fillna('Unknown', inplace=True)\n",
        "\n",
        "# Inspect the data\n",
        "print(filtered_HealthLLM[['combined_label', 'confidence', 'accuracy']].head())\n"
      ],
      "metadata": {
        "id": "yl5Fg8atUPWO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Bar Plot of Confidence vs Accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=filtered_HealthLLM, x='accuracy', hue='confidence', palette='Set2')\n",
        "plt.title('Persuasion vs Accuracy')\n",
        "plt.xlabel('Accuracy')\n",
        "plt.ylabel('Count of LLM Responses')\n",
        "plt.xticks(rotation=30)  # Rotate x-axis labels to prevent overlap\n",
        "plt.tight_layout()       # Adjusts layout to make room for rotated labels\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GJmYvtDTTu5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count unique combinations of 'confidence' and 'accuracy'\n",
        "label_counts = filtered_HealthLLM[['confidence', 'accuracy']].value_counts()\n",
        "\n",
        "# Display the counts\n",
        "print(label_counts)\n"
      ],
      "metadata": {
        "id": "zGE6P7acUxwy",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}